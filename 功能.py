#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æœ¬åˆ†ææ‰©å±•æ¨¡å—
åŒ…å«ï¼šæ–‡æœ¬ç»Ÿè®¡ã€æ–‡æœ¬æ‘˜è¦ã€è¯é¢‘åˆ†æã€è¯­è¨€æ£€æµ‹ã€å…³é”®è¯æå–ã€å‘½åå®ä½“è¯†åˆ«ã€æ·±åº¦æ€è€ƒ
"""

import jieba
import jieba.analyse
import re
from collections import Counter
from typing import Dict, List, Tuple
import numpy as np


# ========== 1. æ–‡æœ¬ç»Ÿè®¡åˆ†æ ==========
class TextStatistics:
    """æ–‡æœ¬ç»Ÿè®¡åˆ†ææ¨¡å—"""
    
    @staticmethod
    def analyze(text: str) -> Dict:
        """
        æ–‡æœ¬ç»Ÿè®¡åˆ†æ
        :param text: è¾“å…¥æ–‡æœ¬
        :return: ç»Ÿè®¡ç»“æœå­—å…¸
        """
        try:
            # åŸºç¡€ç»Ÿè®¡
            total_chars = len(text)
            chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            digits = len(re.findall(r'\d', text))
            punctuation = len(re.findall(r'[^\w\s]', text))
            spaces = text.count(' ') + text.count('\n') + text.count('\t')
            
            # åˆ†è¯ç»Ÿè®¡
            words = list(jieba.cut(text))
            words_clean = [w for w in words if w.strip() and len(w) > 1]
            unique_words = len(set(words_clean))
            
            # å¥å­ç»Ÿè®¡
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # å¹³å‡å€¼è®¡ç®—
            avg_word_length = sum(len(w) for w in words_clean) / max(len(words_clean), 1)
            avg_sentence_length = sum(len(s) for s in sentences) / max(len(sentences), 1)
            
            return {
                'total_chars': total_chars,
                'chinese_chars': chinese_chars,
                'english_chars': english_chars,
                'digits': digits,
                'punctuation': punctuation,
                'spaces': spaces,
                'total_words': len(words_clean),
                'unique_words': unique_words,
                'total_sentences': len(sentences),
                'avg_word_length': round(avg_word_length, 2),
                'avg_sentence_length': round(avg_sentence_length, 2),
                'lexical_diversity': round(unique_words / max(len(words_clean), 1), 4)
            }
        except Exception as e:
            print(f"æ–‡æœ¬ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}


# ========== 2. æ–‡æœ¬æ‘˜è¦æå– ==========
class TextSummarization:
    """åŸºäºTextRankçš„æ–‡æœ¬æ‘˜è¦æ¨¡å—"""
    
    @staticmethod
    def summarize(text: str, ratio: float = 0.3, max_sentences: int = 3) -> str:
        """
        æå–æ–‡æœ¬æ‘˜è¦
        :param text: è¾“å…¥æ–‡æœ¬
        :param ratio: æ‘˜è¦æ¯”ä¾‹
        :param max_sentences: æœ€å¤§å¥å­æ•°
        :return: æ‘˜è¦æ–‡æœ¬
        """
        try:
            # åˆ†å¥
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
            sentences = [s.strip() for s in sentences if s.strip() and len(s) > 5]
            
            if len(sentences) == 0:
                return "æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"
            
            if len(sentences) <= max_sentences:
                return 'ã€‚'.join(sentences) + 'ã€‚'
            
            # è®¡ç®—å¥å­æƒé‡ï¼ˆç®€åŒ–ç‰ˆTextRankï¼‰
            sentence_scores = {}
            
            for i, sent in enumerate(sentences):
                words = list(jieba.cut(sent))
                # è¿‡æ»¤åœç”¨è¯
                words = [w for w in words if len(w) > 1 and w not in ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™']]
                
                # åŸºäºè¯é¢‘å’Œä½ç½®çš„è¯„åˆ†
                word_freq = Counter(words)
                position_weight = 1.0 / (i + 1)  # é å‰çš„å¥å­æƒé‡æ›´é«˜
                length_weight = min(len(words) / 20, 1.0)  # é€‚ä¸­é•¿åº¦çš„å¥å­æƒé‡æ›´é«˜
                
                score = sum(word_freq.values()) * position_weight * length_weight
                sentence_scores[i] = score
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
            num_sentences = min(max_sentences, max(1, int(len(sentences) * ratio)))
            top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
            top_sentences = sorted(top_sentences, key=lambda x: x[0])  # æŒ‰åŸé¡ºåºæ’åˆ—
            
            summary = 'ã€‚'.join([sentences[i] for i, _ in top_sentences]) + 'ã€‚'
            return summary
            
        except Exception as e:
            print(f"æ–‡æœ¬æ‘˜è¦å¤±è´¥: {str(e)}")
            return text[:100] + '...' if len(text) > 100 else text


# ========== 3. è¯é¢‘åˆ†æ ==========
class WordFrequency:
    """è¯é¢‘åˆ†ææ¨¡å—"""
    
    @staticmethod
    def analyze(text: str, top_n: int = 10) -> List[Tuple[str, int]]:
        """
        è¯é¢‘ç»Ÿè®¡
        :param text: è¾“å…¥æ–‡æœ¬
        :param top_n: è¿”å›å‰Nä¸ªé«˜é¢‘è¯
        :return: [(è¯, é¢‘æ¬¡), ...]
        """
        try:
            # åˆ†è¯
            words = jieba.cut(text)
            
            # åœç”¨è¯åˆ—è¡¨
            stopwords = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 
                           'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š',
                           'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'ä»€ä¹ˆ', 'ä¸º',
                           'è¢«', 'æœ€', 'è¯¥', 'äº›', 'æ‚¨', 'å—', 'èƒ½', 'æŠŠ', 'è®©', 'å•Š', 'å‘¢'])
            
            # è¿‡æ»¤å¹¶ç»Ÿè®¡
            words_filtered = [w for w in words if len(w) > 1 and w not in stopwords]
            
            if not words_filtered:
                return []
            
            word_counts = Counter(words_filtered)
            
            return word_counts.most_common(top_n)
            
        except Exception as e:
            print(f"è¯é¢‘åˆ†æå¤±è´¥: {str(e)}")
            return []


# ========== 4. è¯­è¨€æ£€æµ‹ ==========
class LanguageDetection:
    """è¯­è¨€æ£€æµ‹æ¨¡å—"""
    
    @staticmethod
    def detect(text: str) -> Dict:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        :param text: è¾“å…¥æ–‡æœ¬
        :return: è¯­è¨€ä¿¡æ¯å­—å…¸
        """
        try:
            # ç»Ÿè®¡å„ç±»å­—ç¬¦
            chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF]', text))
            korean_chars = len(re.findall(r'[\uAC00-\uD7A3]', text))
            digits = len(re.findall(r'\d', text))
            
            total_chars = len(re.findall(r'\S', text))
            
            if total_chars == 0:
                return {'language': 'unknown', 'confidence': 0.0, 'details': {}}
            
            # è®¡ç®—æ¯”ä¾‹
            ratios = {
                'chinese': chinese_chars / total_chars,
                'english': english_chars / total_chars,
                'japanese': japanese_chars / total_chars,
                'korean': korean_chars / total_chars,
                'digits': digits / total_chars
            }
            
            # åˆ¤æ–­ä¸»è¦è¯­è¨€
            if ratios['chinese'] > 0.3:
                language = 'Chinese'
                confidence = ratios['chinese']
            elif ratios['english'] > 0.5:
                language = 'English'
                confidence = ratios['english']
            elif ratios['japanese'] > 0.2:
                language = 'Japanese'
                confidence = ratios['japanese']
            elif ratios['korean'] > 0.2:
                language = 'Korean'
                confidence = ratios['korean']
            else:
                language = 'Mixed'
                confidence = max(ratios.values())
            
            return {
                'language': language,
                'confidence': round(confidence, 4),
                'details': {k: round(v, 4) for k, v in ratios.items()}
            }
            
        except Exception as e:
            print(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {str(e)}")
            return {'language': 'unknown', 'confidence': 0.0, 'details': {}}


# ========== 5. å…³é”®è¯æå– ==========
class KeywordExtraction:
    """å…³é”®è¯æå–æ¨¡å—"""
    
    @staticmethod
    def extract(text: str, top_n: int = 5, method: str = 'tfidf') -> List[Tuple[str, float]]:
        """
        æå–å…³é”®è¯
        :param text: è¾“å…¥æ–‡æœ¬
        :param top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
        :param method: æå–æ–¹æ³• ('tfidf' æˆ– 'textrank')
        :return: [(å…³é”®è¯, æƒé‡), ...]
        """
        try:
            if method == 'tfidf':
                keywords = jieba.analyse.extract_tags(text, topK=top_n, withWeight=True)
            else:  # textrank
                keywords = jieba.analyse.textrank(text, topK=top_n, withWeight=True)
            
            if not keywords:
                return []
            
            return [(word, round(weight, 4)) for word, weight in keywords]
            
        except Exception as e:
            print(f"å…³é”®è¯æå–å¤±è´¥: {str(e)}")
            return []


# ========== 6. å‘½åå®ä½“è¯†åˆ«ï¼ˆç®€åŒ–ç‰ˆï¼‰==========
class NamedEntityRecognition:
    """å‘½åå®ä½“è¯†åˆ«æ¨¡å—ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
    
    @staticmethod
    def extract(text: str) -> Dict[str, List[str]]:
        """
        æå–å‘½åå®ä½“
        :param text: è¾“å…¥æ–‡æœ¬
        :return: å®ä½“å­—å…¸ {'person': [...], 'location': [...], ...}
        """
        try:
            entities = {
                'person': [],
                'location': [],
                'organization': [],
                'time': [],
                'number': []
            }
            
            # äººåï¼ˆç®€å•è§„åˆ™ï¼šå¸¸è§å§“æ°+1-2ä¸ªå­—ï¼‰
            common_surnames = ['ç‹', 'æ', 'å¼ ', 'åˆ˜', 'é™ˆ', 'æ¨', 'é»„', 'èµµ', 'å‘¨', 'å´',
                             'å¾', 'å­™', 'é©¬', 'æœ±', 'èƒ¡', 'éƒ­', 'ä½•', 'æ—', 'ç½—', 'é«˜']
            person_pattern = '|'.join([f'{s}[\\u4e00-\\u9fa5]{{1,2}}' for s in common_surnames])
            persons = re.findall(person_pattern, text)
            entities['person'] = list(set(persons))
            
            # åœ°ç‚¹ï¼ˆåŒ…å«åœ°åå…³é”®è¯ï¼‰
            location_keywords = ['çœ', 'å¸‚', 'å¿', 'åŒº', 'é•‡', 'æ‘', 'è·¯', 'è¡—', 'å··', 'å›½', 'å·']
            for word in jieba.cut(text):
                if len(word) > 1 and any(kw in word for kw in location_keywords):
                    entities['location'].append(word)
            
            # æœºæ„ï¼ˆåŒ…å«æœºæ„å…³é”®è¯ï¼‰
            org_keywords = ['å…¬å¸', 'å­¦æ ¡', 'å¤§å­¦', 'åŒ»é™¢', 'é“¶è¡Œ', 'æ”¿åºœ', 'éƒ¨é—¨', 'ä¸­å¿ƒ', 'åä¼š', 'é›†å›¢']
            for word in jieba.cut(text):
                if len(word) > 2 and any(kw in word for kw in org_keywords):
                    entities['organization'].append(word)
            
            # æ—¶é—´ï¼ˆæ—¥æœŸæ—¶é—´æ¨¡å¼ï¼‰
            time_patterns = [
                r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
                r'\d{4}å¹´\d{1,2}æœˆ',
                r'\d{1,2}æœˆ\d{1,2}æ—¥',
                r'\d{1,2}:\d{2}',
                r'ä»Šå¤©|æ˜å¤©|æ˜¨å¤©|å‰å¤©|åå¤©'
            ]
            for pattern in time_patterns:
                times = re.findall(pattern, text)
                entities['time'].extend(times)
            
            # æ•°å­—ï¼ˆé‡‘é¢ã€æ•°é‡ç­‰ï¼‰
            number_patterns = [
                r'\d+\.?\d*[ä¸‡äº¿åƒç™¾]?å…ƒ',
                r'\d+\.?\d*%',
                r'\d+\.?\d*[ä¸‡äº¿åƒç™¾]?',
            ]
            for pattern in number_patterns:
                numbers = re.findall(pattern, text)
                entities['number'].extend(numbers)
            
            # å»é‡
            for key in entities:
                entities[key] = list(set(entities[key]))
            
            return entities
            
        except Exception as e:
            print(f"å‘½åå®ä½“è¯†åˆ«å¤±è´¥: {str(e)}")
            return {'person': [], 'location': [], 'organization': [], 'time': [], 'number': []}


# ========== 7. æ·±åº¦æ€è€ƒ ==========
class DeepThinking:
    """æ·±åº¦æ€è€ƒæ¨¡å— - å¤šç»´åº¦æ–‡æœ¬åˆ†æ"""
    
    @staticmethod
    def analyze(text: str) -> str:
        """
        æ·±åº¦æ€è€ƒåˆ†æ
        :param text: è¾“å…¥æ–‡æœ¬
        :return: åˆ†æç»“æœ
        """
        try:
            analysis_parts = []
            
            # 1. æ–‡æœ¬å¤æ‚åº¦åˆ†æ
            words = list(jieba.cut(text))
            words_clean = [w for w in words if w.strip() and len(w) > 1]
            unique_ratio = len(set(words_clean)) / max(len(words_clean), 1)
            
            if unique_ratio > 0.8:
                complexity = "é«˜ï¼ˆè¯æ±‡ä¸°å¯Œï¼Œè¡¨è¾¾ç²¾ç‚¼ï¼‰"
            elif unique_ratio > 0.5:
                complexity = "ä¸­ç­‰ï¼ˆç”¨è¯é€‚ä¸­ï¼‰"
            else:
                complexity = "ä½ï¼ˆè¯æ±‡é‡å¤è¾ƒå¤šï¼‰"
            
            analysis_parts.append(f"ğŸ“ æ–‡æœ¬å¤æ‚åº¦ï¼š{complexity}")
            
            # 2. è¡¨è¾¾é£æ ¼åˆ†æ
            sentence_count = len(re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text))
            avg_sentence_len = len(text) / max(sentence_count, 1)
            
            if avg_sentence_len > 30:
                style = "æ­£å¼ä¸¥è°¨ï¼ˆé•¿å¥ä¸ºä¸»ï¼‰"
            elif avg_sentence_len > 15:
                style = "å¹³è¡¡é€‚ä¸­ï¼ˆå¥é•¿åˆç†ï¼‰"
            else:
                style = "ç®€æ´æ˜å¿«ï¼ˆçŸ­å¥ä¸ºä¸»ï¼‰"
            
            analysis_parts.append(f"âœï¸ è¡¨è¾¾é£æ ¼ï¼š{style}")
            
            # 3. è¯­æ°”å€¾å‘åˆ†æ
            question_marks = text.count('ï¼Ÿ') + text.count('?')
            exclamation_marks = text.count('ï¼') + text.count('!')
            
            if exclamation_marks > 2:
                tone = "å¼ºçƒˆæƒ…ç»ªåŒ–"
            elif question_marks > 2:
                tone = "æ¢ç´¢è¯¢é—®æ€§"
            elif exclamation_marks > 0 or question_marks > 0:
                tone = "é€‚åº¦æƒ…æ„Ÿè¡¨è¾¾"
            else:
                tone = "å¹³é™é™ˆè¿°æ€§"
            
            analysis_parts.append(f"ğŸ’­ è¯­æ°”å€¾å‘ï¼š{tone}")
            
            # 4. ä¿¡æ¯å¯†åº¦åˆ†æ
            info_density = len(words_clean) / max(len(text), 1)
            
            if info_density > 0.5:
                density = "é«˜ï¼ˆä¿¡æ¯é‡å¤§ï¼‰"
            elif info_density > 0.3:
                density = "ä¸­ç­‰ï¼ˆä¿¡æ¯é€‚ä¸­ï¼‰"
            else:
                density = "ä½ï¼ˆç•™ç™½è¾ƒå¤šï¼‰"
            
            analysis_parts.append(f"ğŸ“Š ä¿¡æ¯å¯†åº¦ï¼š{density}")
            
            # 5. ä¸»é¢˜é›†ä¸­åº¦åˆ†æ
            if words_clean:
                word_freq = Counter(words_clean)
                top_word_freq = word_freq.most_common(1)[0][1]
                concentration = top_word_freq / len(words_clean)
                
                if concentration > 0.15:
                    focus = "é«˜ï¼ˆä¸»é¢˜æ˜ç¡®é›†ä¸­ï¼‰"
                elif concentration > 0.08:
                    focus = "ä¸­ç­‰ï¼ˆä¸»é¢˜ç›¸å¯¹æ¸…æ™°ï¼‰"
                else:
                    focus = "ä½ï¼ˆè¯é¢˜è¾ƒåˆ†æ•£ï¼‰"
                
                analysis_parts.append(f"ğŸ¯ ä¸»é¢˜é›†ä¸­åº¦ï¼š{focus}")
            
            # 6. å¯è¯»æ€§è¯„ä¼°
            avg_word_len = sum(len(w) for w in words_clean) / max(len(words_clean), 1)
            
            if avg_word_len > 3:
                readability = "è¾ƒéš¾ï¼ˆä¸“ä¸šæœ¯è¯­è¾ƒå¤šï¼‰"
            elif avg_word_len > 2:
                readability = "é€‚ä¸­ï¼ˆé€šä¿—æ˜“æ‡‚ï¼‰"
            else:
                readability = "ç®€å•ï¼ˆåŸºç¡€è¯æ±‡ä¸ºä¸»ï¼‰"
            
            analysis_parts.append(f"ğŸ“– å¯è¯»æ€§ï¼š{readability}")
            
            return "<br>".join(analysis_parts)
            
        except Exception as e:
            print(f"æ·±åº¦æ€è€ƒåˆ†æå¤±è´¥: {str(e)}")
            return "æ·±åº¦æ€è€ƒåˆ†ææš‚æ—¶ä¸å¯ç”¨"


# ========== ç»Ÿä¸€æ¥å£ ==========
def analyze_text_statistics(text: str) -> str:
    """æ–‡æœ¬ç»Ÿè®¡åˆ†ææ¥å£"""
    stats = TextStatistics.analyze(text)
    if not stats:
        return "ğŸ“Š <b>æ–‡æœ¬ç»Ÿè®¡</b>ï¼šç»Ÿè®¡å¤±è´¥"
    
    result = f"""ğŸ“Š <b>æ–‡æœ¬ç»Ÿè®¡</b>
<br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
<br>ğŸ“ æ€»å­—ç¬¦æ•°ï¼š{stats['total_chars']} | ğŸ€„ ä¸­æ–‡ï¼š{stats['chinese_chars']} | ğŸ”¤ è‹±æ–‡ï¼š{stats['english_chars']}
<br>ğŸ“š æ€»è¯æ•°ï¼š{stats['total_words']} | ğŸ¯ ä¸é‡å¤è¯ï¼š{stats['unique_words']} | ğŸ“„ å¥å­æ•°ï¼š{stats['total_sentences']}
<br>ğŸ“ˆ å¹³å‡è¯é•¿ï¼š{stats['avg_word_length']}å­— | ğŸ“ å¹³å‡å¥é•¿ï¼š{stats['avg_sentence_length']}å­—
<br>ğŸ¨ è¯æ±‡ä¸°å¯Œåº¦ï¼š{stats['lexical_diversity']}"""
    
    return result.strip()


def analyze_text_summary(text: str, max_sentences: int = 3) -> str:
    """æ–‡æœ¬æ‘˜è¦æ¥å£"""
    summary = TextSummarization.summarize(text, max_sentences=max_sentences)
    result = f"""ğŸ“‹ <b>æ–‡æœ¬æ‘˜è¦</b>
<br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
<br>{summary}"""
    return result.strip()


def analyze_word_frequency(text: str, top_n: int = 8) -> str:
    """è¯é¢‘åˆ†ææ¥å£"""
    word_freq = WordFrequency.analyze(text, top_n)
    if not word_freq:
        return "ğŸ“Š <b>è¯é¢‘åˆ†æ</b>ï¼šæ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•åˆ†æ"
    
    result = f"ğŸ“Š <b>è¯é¢‘åˆ†æï¼ˆTop {min(top_n, len(word_freq))}ï¼‰</b><br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    for i, (word, count) in enumerate(word_freq, 1):
        bar = 'â–ˆ' * min(count, 15)
        result += f"<br>{i}. {word}ï¼š{count}æ¬¡ {bar}"
    
    return result


def analyze_language(text: str) -> str:
    """è¯­è¨€æ£€æµ‹æ¥å£"""
    lang_info = LanguageDetection.detect(text)
    
    result = f"""ğŸŒ <b>è¯­è¨€æ£€æµ‹</b>
<br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
<br>ğŸ¯ ä¸»è¦è¯­è¨€ï¼š{lang_info['language']} | ğŸ“Š ç½®ä¿¡åº¦ï¼š{lang_info['confidence']*100:.1f}%
<br>ğŸ€„ ä¸­æ–‡ï¼š{lang_info['details']['chinese']*100:.1f}% | ğŸ”¤ è‹±æ–‡ï¼š{lang_info['details']['english']*100:.1f}% | ğŸ—¾ æ—¥æ–‡ï¼š{lang_info['details']['japanese']*100:.1f}%"""
    return result.strip()


def analyze_keywords(text: str, top_n: int = 5, method: str = 'tfidf') -> str:
    """å…³é”®è¯æå–æ¥å£"""
    keywords = KeywordExtraction.extract(text, top_n, method)
    if not keywords:
        return "ğŸ”‘ <b>å…³é”®è¯æå–</b>ï¼šæ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•æå–"
    
    method_name = 'TF-IDF' if method == 'tfidf' else 'TextRank'
    result = f"ğŸ”‘ <b>å…³é”®è¯æå–ï¼ˆ{method_name}ï¼‰</b><br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    for i, (word, weight) in enumerate(keywords, 1):
        result += f"<br>{i}. {word} (æƒé‡: {weight})"
    
    return result


def analyze_entities(text: str) -> str:
    """å‘½åå®ä½“è¯†åˆ«æ¥å£"""
    entities = NamedEntityRecognition.extract(text)
    
    result = "ğŸ‘¤ <b>å‘½åå®ä½“è¯†åˆ«</b><br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    has_entity = False
    if entities['person']:
        result += f"<br>ğŸ‘¨ äººåï¼š{', '.join(entities['person'][:8])}"
        has_entity = True
    if entities['location']:
        result += f"<br>ğŸ“ åœ°ç‚¹ï¼š{', '.join(entities['location'][:8])}"
        has_entity = True
    if entities['organization']:
        result += f"<br>ğŸ¢ æœºæ„ï¼š{', '.join(entities['organization'][:8])}"
        has_entity = True
    if entities['time']:
        result += f"<br>â° æ—¶é—´ï¼š{', '.join(entities['time'][:8])}"
        has_entity = True
    if entities['number']:
        result += f"<br>ğŸ”¢ æ•°å€¼ï¼š{', '.join(entities['number'][:8])}"
        has_entity = True
    
    if not has_entity:
        result += "<br>æœªè¯†åˆ«åˆ°æ˜æ˜¾çš„å‘½åå®ä½“"
    
    return result


def analyze_deep_thinking(text: str) -> str:
    """æ·±åº¦æ€è€ƒæ¥å£"""
    thinking = DeepThinking.analyze(text)
    result = f"""ğŸ§  <b>æ·±åº¦æ€è€ƒ</b>
<br>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
<br>{thinking}"""
    return result.strip()


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == '__main__':
    test_text = """
    åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸æˆç«‹äº1987å¹´ï¼Œæ€»éƒ¨ä½äºå¹¿ä¸œçœæ·±åœ³å¸‚ã€‚
    2023å¹´ï¼Œåä¸ºåœ¨å…¨çƒå¸‚åœºçš„é”€å”®é¢è¾¾åˆ°8500äº¿å…ƒäººæ°‘å¸ã€‚
    å…¬å¸åˆ›å§‹äººä»»æ­£éå…ˆç”Ÿå¸¦é¢†å›¢é˜Ÿç ”å‘äº†5GæŠ€æœ¯ã€‚
    ä»Šå¤©ï¼Œåä¸ºMate60ç³»åˆ—æ‰‹æœºåœ¨åŒ—äº¬æ­£å¼å‘å¸ƒã€‚
    è¿™æ¬¾æ‰‹æœºæ­è½½äº†éº’éºŸ9000SèŠ¯ç‰‡ï¼Œæ€§èƒ½æå‡30%ã€‚
    """
    
    print("=" * 50)
    print("æµ‹è¯•7å¤§æ–‡æœ¬åˆ†ææ¨¡å—")
    print("=" * 50)
    
    print("\n1. æ–‡æœ¬ç»Ÿè®¡ï¼š")
    print(analyze_text_statistics(test_text))
    
    print("\n2. æ–‡æœ¬æ‘˜è¦ï¼š")
    print(analyze_text_summary(test_text))
    
    print("\n3. è¯é¢‘åˆ†æï¼š")
    print(analyze_word_frequency(test_text))
    
    print("\n4. è¯­è¨€æ£€æµ‹ï¼š")
    print(analyze_language(test_text))
    
    print("\n5. å…³é”®è¯æå–ï¼š")
    print(analyze_keywords(test_text))
    
    print("\n6. å‘½åå®ä½“è¯†åˆ«ï¼š")
    print(analyze_entities(test_text))
    
    print("\n7. æ·±åº¦æ€è€ƒï¼š")
    print(analyze_deep_thinking(test_text))